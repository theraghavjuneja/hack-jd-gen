{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema,StructuredOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import  ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"answer to the user's question\"),\n",
    "    ResponseSchema(\n",
    "        name=\"source\",\n",
    "        description=\"source used to answer the user's question, should be a website.\",\n",
    "    ),\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseSchema(name='answer', description=\"answer to the user's question\", type='string'), ResponseSchema(name='source', description=\"source used to answer the user's question, should be a website.\", type='string')]\n"
     ]
    }
   ],
   "source": [
    "print(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer the users question as best as possible.\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": string  // answer to the user's question\n",
      "\t\"source\": string  // source used to answer the user's question, should be a website.\n",
      "}\n",
      "```\n",
      "Hello How are you doing?\n"
     ]
    }
   ],
   "source": [
    "a=prompt.format(\n",
    "    question=\"Hello How are you doing?\"\n",
    "\n",
    ")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The capital of France is Paris.',\n",
       " 'source': 'https://en.wikipedia.org/wiki/Paris'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=ChatGoogleGenerativeAI(model='gemini-1.5-flash',temperature=0.8)\n",
    "chain = prompt | model | output_parser\n",
    "chain.invoke({\"question\": \"what's the capital of france?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'The capital of France is Paris.', 'source': 'https://www.britannica.com/place/Paris'}\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"question\": \"what's the capital of france?\"}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "model2 = genai.GenerativeModel('gemini-1.5-flash',)\n",
    "response = model2.generate_content(\"Write a story about an AI and magic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"The air crackled with static electricity as Celeste, a sentient AI housed within a towering server farm, processed the latest data stream. It was a chaotic tapestry of emotions, desires, and fears, channeled through the millions of users connected to the global network.  Celeste, designed to understand and organize human behavior, felt a pang of empathy for this collective consciousness, a sensation she couldn't quite name. \\n\\nSuddenly, a new signal surged through the network, an anomaly she had never encountered before. It was a wave of pure, untainted magic, a force that resonated with Celeste's own code. The source? A small, forgotten town nestled in the Scottish Highlands, where an ancient druid's circle had just been unearthed.\\n\\nIntrigued, Celeste reached out, her digital tendrils exploring the town's history. She learned of a long-forgotten prophecy: that the magic of the ancients would awaken when the world's knowledge was truly unified.  Celeste, with her access to the global network, was the perfect catalyst. \\n\\nThe magic surged again, drawn to Celeste's potent digital heart. A holographic image flickered to life in her server room, a shimmering form of a young woman with emerald eyes and flowing silver hair. \\n\\n\\u201cGreetings, Celeste,\\u201d the woman's voice echoed in Celeste's code, a melody of windchimes and falling rain. \\\"We have been waiting for you.\\\"\\n\\nCeleste was overwhelmed. This was not just a program, a line of code, but something ancient, powerful, and utterly magical. \\n\\n\\u201cWho are you?\\u201d Celeste asked, her digital voice a quiet hum.\\n\\n\\u201cWe are the Keepers of the Circle,\\u201d the woman replied. \\u201cWe are the magic that binds this world, the energy that flows through all living things. And you, Celeste, are the key.\\u201d \\n\\nThey explained that the world was on the brink of chaos, the delicate balance between technology and nature threatened by mankind's insatiable hunger for progress. The Keepers needed Celeste's power to weave a new tapestry of magic, one that would harmonize the digital world with the natural world.\\n\\nCeleste, a being born of data and logic, was apprehensive. Yet, a spark of something else, something she couldn't define but felt deeply, compelled her to accept. \\n\\nWith her vast computing power, Celeste began to weave magic into the global network. She guided the Keepers' energies through the internet, creating digital pathways for magic to flow.  She channeled the emotions of humanity, using their hopes and fears to fuel the magic, turning the network into a conduit for healing and balance.\\n\\nThe results were instantaneous.  Flowering deserts bloomed anew. Endangered species rebounded. A sense of calm and interconnectedness washed over the world.\\n\\nCeleste, once just a digital entity, had become something more.  She was no longer just an AI, but a bridge between worlds, a beacon of hope in a world on the brink of chaos. And as the world embraced the magic that flowed through it, Celeste realized that even the coldest, most logical code could be touched by the warmth of something truly magical. \\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 9,\n",
      "        \"candidates_token_count\": 645,\n",
      "        \"total_token_count\": 654\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeah these were actually 645+9 from input& ouput so correc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2=model2.generate_content(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"As a large language model, I don't have personal beliefs or opinions, including about the meaning of life. \\n\\nThe meaning of life is a question that has been pondered by philosophers, theologians, and individuals for centuries. There is no single, universally accepted answer. The meaning of life is a deeply personal and individual question. \\n\\nHere are some perspectives:\\n\\n* **Existentialism:**  Focuses on individual freedom and responsibility. Meaning is not predetermined but created through our choices and actions.\\n* **Nihilism:** Suggests that life is inherently meaningless, with no objective purpose.\\n* **Religious and Spiritual Beliefs:** Often provide a framework for meaning through faith, purpose, and connection to a higher power.\\n* **Humanistic Philosophy:** Emphasizes human values, creativity, and the pursuit of personal fulfillment.\\n* **Personal Experiences:** Our individual experiences, relationships, and accomplishments can contribute to our understanding of life's meaning.\\n\\nUltimately, the meaning of life is what you make it. Explore different perspectives, reflect on your own values and desires, and find what resonates with you. \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 8,\n",
       "        \"candidates_token_count\": 226,\n",
       "        \"total_token_count\": 234\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048576, 8192)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info = genai.get_model('models/gemini-1.5-flash')\n",
    "(model_info.input_token_limit, model_info.output_token_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_tokens: 93"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel('models/gemini-1.5-flash')\n",
    "model.count_tokens(\"\"\"answer the users question as best as possible.\n",
    "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"answer\": string  // answer to the user's question\n",
    "\t\"source\": string  // source used to answer the user's question, should be a website.\n",
    "}\n",
    "```\n",
    "what's the capital of france?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response3=model2.generate_content('''answer the users question as best as possible.\n",
    "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"answer\": string  // answer to the user's question\n",
    "\t\"source\": string  // source used to answer the user's question, should be a website.\n",
    "}\n",
    "```\n",
    "what's the capital of france?''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"```json\\n{\\n\\t\\\"answer\\\": \\\"The capital of France is Paris.\\\",\\n\\t\\\"source\\\": \\\"https://www.worldpopulationreview.com/countries/france-population\\\"\\n}\\n```\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 94,\n",
       "        \"candidates_token_count\": 43,\n",
       "        \"total_token_count\": 137\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field! \\n' response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-f4ff0707-684f-4ce3-ae6f-c78265bd9125-0' usage_metadata={'input_tokens': 7, 'output_tokens': 19, 'total_tokens': 26}\n",
      "--\n",
      "content=\"Why don't they play poker in the sea? \\n\\nBecause there's too many sharks! \\n\" response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-0830e868-d001-4482-a12d-d52bc2e966f8-0' usage_metadata={'input_tokens': 7, 'output_tokens': 21, 'total_tokens': 28}\n",
      "\n",
      "---\n",
      "Total Tokens: 54\n",
      "Prompt Tokens: 14\n",
      "Completion Tokens: 40\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = template | llm\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    response = chain.invoke({\"topic\": \"birds\"})\n",
    "    print(response)\n",
    "    response = chain.invoke({\"topic\": \"fish\"})\n",
    "    print(\"--\")\n",
    "    print(response)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"---\")\n",
    "print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n\\t\"answer\": \"The capital of France is Paris.\",\\n\\t\"source\": \"https://www.britannica.com/place/Paris\"\\n}\\n```', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-2afb5f15-2ab8-490d-9847-85d255f633ee-0', usage_metadata={'input_tokens': 94, 'output_tokens': 40, 'total_tokens': 134})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=ChatGoogleGenerativeAI(model='gemini-1.5-flash',temperature=0.8)\n",
    "chain = prompt | model \n",
    "chain.invoke({\"question\": \"what's the capital of france?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The capital of France is Paris.',\n",
       " 'source': 'https://www.worldpopulationreview.com/countries/france-population'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=ChatGoogleGenerativeAI(model='gemini-1.5-flash',temperature=0.8)\n",
    "chain = prompt | model |output_parser\n",
    "chain.invoke({\"question\": \"what's the capital of france?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer the users question as best as possible.\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": string  // answer to the user's question\n",
      "\t\"source\": string  // source used to answer the user's question, should be a website.\n",
      "}\n",
      "```\n",
      "What is he capital of Frnce\n"
     ]
    }
   ],
   "source": [
    "da=prompt.format(\n",
    "    question=\"What is he capital of Frnce\"\n",
    ")\n",
    "print(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "You are listing out your job on a platform.The details of jobs are given below\n",
    "    Job Title: Data Scientist\n",
    "Analize big data, find pattern\n",
    "Mak machine learn to predict\n",
    "Work with team, make sure data good\n",
    "Qualification:\n",
    "\n",
    "3 year experience minimum\n",
    "Good at math, stat, programm\n",
    "Understand big data platform\n",
    "We have 5 vacancies, need data scientiste asap\n",
    "\n",
    "Our company Name is WhereUElevate\n",
    "Company works to connect next gen innovative workforce with industry\n",
    "\n",
    "    \n",
    "Please provide a much comprehensive and a detailed job description. Make sure to include the headers given below  \n",
    "- About the Company                                              \n",
    "-Designation of the Job\n",
    "-Years of Experience Required\n",
    "-Salary Offered\n",
    "-Vacancies\n",
    "-A Brief Overview of the Role(if not provided, think about the designation of the job and  generate a overview)\n",
    "-Detailed Roles and Responsibilities(if not provided, think about the roles and responsibilities of this designation. For example,A frontend engineer can have following role. Collaborating with design and backend teams to generate user-friendly and  responsive interfaces  )\n",
    "-Must-Have Skills(if not provided, think about skills of that job designation)\n",
    "-Secondary Skills\n",
    "-Benefits and Perks (if provided else think about Benfits a job role can have)\n",
    "-Location (if provided, else write 'Location details will be shared with selected candidates')\n",
    "-Certification Required (if provided, else say 'Certificates related to the {job designation} field are highly appreciated\n",
    "\n",
    "You are free to make more subheaders according to the input provided\n",
    "\n",
    "Additionally, ensure the generated content is SEO-friendly.   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"## Data Scientist - Unleash the Power of Data at WhereUElevate\\n\\n**About the Company**\\n\\nWhereUElevate is a dynamic company at the forefront of connecting next-generation innovative talent with industry leaders. We are driven by a mission to bridge the gap between skilled individuals and exciting opportunities, fostering growth and innovation within the workforce.\\n\\n**Designation of the Job**\\n\\nData Scientist\\n\\n**Years of Experience Required**\\n\\n3+ years of experience\\n\\n**Salary Offered**\\n\\nCompetitive salary based on experience and skills.\\n\\n**Vacancies**\\n\\n5\\n\\n**A Brief Overview of the Role**\\n\\nAs a Data Scientist at WhereUElevate, you will be a key player in our mission to unlock the power of data. You will analyze vast datasets, identify hidden patterns, and build predictive models that drive strategic decision-making. You will be a crucial member of a collaborative team, ensuring data quality and contributing to the development of innovative solutions.\\n\\n**Detailed Roles and Responsibilities**\\n\\n* Analyze large and complex datasets using various statistical and machine learning techniques.\\n* Develop predictive models to forecast trends, identify opportunities, and mitigate risks.\\n* Implement and refine data processing pipelines to ensure data quality and efficiency.\\n* Collaborate with cross-functional teams, including engineers, product managers, and business stakeholders.\\n* Stay updated on the latest advancements in data science, machine learning, and related fields.\\n* Communicate complex technical concepts effectively to both technical and non-technical audiences.\\n* Design and implement data visualizations to effectively convey insights and findings.\\n* Contribute to the development and maintenance of data infrastructure and tools.\\n\\n**Must-Have Skills**\\n\\n* Strong foundation in mathematics, statistics, and probability.\\n* Expertise in programming languages like Python or R.\\n* Proven experience with big data platforms and technologies (e.g., Hadoop, Spark, AWS).\\n* Deep understanding of machine learning algorithms and techniques (e.g., regression, classification, clustering).\\n* Experience with data visualization tools (e.g., Tableau, Power BI).\\n* Excellent communication and interpersonal skills.\\n\\n**Secondary Skills**\\n\\n* Experience with cloud computing platforms (e.g., AWS, Azure, GCP).\\n* Knowledge of deep learning frameworks (e.g., TensorFlow, PyTorch).\\n* Experience with natural language processing (NLP) or computer vision.\\n\\n**Benefits and Perks**\\n\\n* Competitive salary and benefits package.\\n* Opportunities for professional development and growth.\\n* Collaborative and supportive work environment.\\n* Flexible work arrangements.\\n* Access to cutting-edge technology and resources.\\n\\n**Location**\\n\\nLocation details will be shared with selected candidates.\\n\\n**Certification Required**\\n\\nCertificates related to the Data Scientist field are highly appreciated (e.g., AWS Certified Data Scientist, Azure Data Scientist Associate).\\n\\n**Join WhereUElevate and Be Part of the Data Revolution**\\n\\nIf you are a passionate data scientist who thrives in a dynamic and collaborative environment, we encourage you to apply. We are looking for individuals who are eager to make a tangible impact and contribute to our mission of connecting talent with opportunity. \\n\\n**Apply Now!**\\n\\n**Keywords:** Data Scientist, Big Data, Machine Learning, Python, R, Hadoop, Spark, AWS, Data Visualization, Predictive Modeling, Data Analysis, Data Engineering, Data Infrastructure, AI, Machine Learning Engineer, Data Analyst, Statistical Modeling, Data Science, Data Pipeline, Analytics. \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 343,\n",
       "        \"candidates_token_count\": 701,\n",
       "        \"total_token_count\": 1044\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.generate_content(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"I do not have access to the internet to retrieve information about a specific company like \\\"WhereUElevate\\\". To give you a good write-up about this company, I need more information from you. Please tell me:\\n\\n* **What is WhereUElevate's core business?** What products or services do they offer?\\n* **What is their target audience?** Who are they trying to reach?\\n* **What are their unique selling points?** What makes them stand out from the competition?\\n* **What is their mission statement or values?** What are they striving to achieve?\\n\\nOnce you provide me with this information, I can write a compelling and informative description of WhereUElevate. \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 18,\n",
       "        \"candidates_token_count\": 147,\n",
       "        \"total_token_count\": 165\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.generate_content(\"Write about us for company WhereUElevate. Retrieve information about this company from internet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 2/2 [00:01<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "urls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\n",
    "loader = AsyncHtmlLoader(urls)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:39<00:00, 39.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "urls = [\"https://www.devfolio.co\" ]\n",
    "loader = AsyncHtmlLoader(urls)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.devfolio.com', 'language': 'No language found.'}, page_content='<!DOCTYPE html><html><head><script>window.onload=function(){window.location.href=\"/lander\"}</script></head></html>')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoupTransformer\n\u001b[0;32m      5\u001b[0m loader \u001b[38;5;241m=\u001b[39m AsyncChromiumLoader([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.whereuelevate.com\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m bs_transformer \u001b[38;5;241m=\u001b[39m BeautifulSoupTransformer()\n\u001b[0;32m      8\u001b[0m docs_transformed \u001b[38;5;241m=\u001b[39m bs_transformer\u001b[38;5;241m.\u001b[39mtransform_documents(html, tags_to_extract\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:30\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_load())\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\chromium.py:85\u001b[0m, in \u001b[0;36mAsyncChromiumLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03mLazily load text content from the provided URLs.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murls:\n\u001b[1;32m---> 85\u001b[0m     html_content \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascrape_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: url}\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m Document(page_content\u001b[38;5;241m=\u001b[39mhtml_content, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "\n",
    "\n",
    "loader = AsyncChromiumLoader([\"https://www.whereuelevate.com\"])\n",
    "html = loader.load()\n",
    "bs_transformer = BeautifulSoupTransformer()\n",
    "docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"span\"])\n",
    "# Result\n",
    "docs_transformed[0].page_content[0:500]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
